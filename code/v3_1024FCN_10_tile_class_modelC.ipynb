{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tile level classifier - Tiles generated from WSI/SVS slides of TCGA dataset\n",
    "# A multi classes image classifier, based on convolutional neural network using Keras and Tensorflow. \n",
    "# A multi-label classifier (having one fully-connected layer at the end), with multi-classification (10 classes, in this instance)\n",
    "# Largely copied from the code https://github.com/kallooa/MSDA_Capstone_Final/tree/master/3_Model_Training/Tile_Level_Model_Training\n",
    "# classifying 10 cancer types (acc, blca, brca, cesc, coad, chol, dlbc, lgg, gbm and read tiles generated from around 400-1200 slides (each class)\n",
    "# with each slide having around 270,000 tiles) downloaded from candygram.emory.edu\n",
    "# Thumbnail extraction and tile extraction from WSI slides are implemented using HistomicsTK (https://girder.readthedocs.io/en/latest/)\n",
    "# Used Keras.ImageDataGenerator for Training/Validation data augmentation and the augmented images are flown from respective directory\n",
    "# train folder has training images and test folder has test images\n",
    "# val folder has validation images, autogenerated by keras \n",
    "# Environment: A docker container having Keras, TensorFlow, Python-3 with GPU based execution\n",
    "\n",
    "# 1024 - FCNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Convolution2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import adagrad, adadelta, rmsprop, adam\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "import datetime, time, os, sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from log_utils import setup_logging_to_file, log_exception # catch and logging exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run Multi-GPU \n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp:\n",
    "now = datetime.datetime.now()\n",
    "filetime = str(now.year)+str(now.month)+str(now.day)+'_'+str(now.hour)+str(now.minute)\n",
    "\n",
    "# Logging to a file\n",
    "log_file = \"v3_1024FCN_10_class_\"+ filetime +\".log\"\n",
    "setup_logging_to_file(log_file)\n",
    "f = open(log_file, \"a\")\n",
    "\n",
    "# User Input\n",
    "# Image dimension:\n",
    "img_width, img_height = 150,150\n",
    "# Epochs\n",
    "epochs = 25\n",
    "# Batch size:\n",
    "batch_size = 25\n",
    "colormode =\"rgb\"\n",
    "channels=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Images Locations\n",
    "training_dir = '/data/train'\n",
    "test_dir = '/data/test'\n",
    "\n",
    "# Results Location:\n",
    "results_dir =\"/data/results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Image Statistics:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count training images:\n",
    "ntraining = 0\n",
    "for root, dirs, files in os.walk(training_dir):\n",
    "    ntraining += len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data format:\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class_weights for unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining class_weights for unbalanced classe\n",
    "class_weight = {0:2.64, 1:1., 2:2.27, 3:1.73, 4:6.79, 5:5.40, 6:5.69, 7:2.50, 8:1.71, 9:6.21}\n",
    "#class_weight = {'acc':2.64, 'blca':1., 'brca':2.27, 'cesc':1.73, 'chol':6.79, 'coad':5.40, 'dlbc':5.69, 'gbm':2.50, 'lgg':1.71, 'read':6.21}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 119530 images belonging to 10 classes.\n",
      "Found 29878 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "# Training Data Generator with Augmentation:\n",
    "# -Scale\n",
    "# -Shear\n",
    "# -Zoom\n",
    "# -Height and Width Shift\n",
    "# -Fill: Nearest\n",
    "# -Horizontal Flip\n",
    "train_datagen = ImageDataGenerator(vertical_flip=True, horizontal_flip=True, validation_split=0.2)\n",
    "\n",
    "# Validation Data Generator:\n",
    "val_datagen = ImageDataGenerator(validation_split=0.2)\n",
    "\n",
    "# Training Data flow from directory:\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(150,150),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=colormode,\n",
    "    subset='training')\n",
    "\n",
    "# Validation Data flow from directory:\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    training_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=colormode,\n",
    "    subset='validation')\n",
    "\n",
    "# Number of Classes/Labels:\n",
    "nLabels = len(val_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0,\n",
       " 'blca': 1,\n",
       " 'brca': 2,\n",
       " 'cesc': 3,\n",
       " 'chol': 4,\n",
       " 'coad': 5,\n",
       " 'dlbc': 6,\n",
       " 'gbm': 7,\n",
       " 'lgg': 8,\n",
       " 'read': 9}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer\n",
    "predictions = Dense(nLabels, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# ompile the model (should be done *after* setting layers to non-trainable)\n",
    "# create model with for binary output with the adam optimization algorithm\n",
    "#model.compile(optimizer='adadelta', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "# This assumes that your machine has 2 available GPUs.\n",
    "parallel_model = multi_gpu_model(model, gpus=2)\n",
    "parallel_model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid broken stream or IO error for >=100k images\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5976/5976 [==============================] - 5253s 879ms/step - loss: 3.1094 - acc: 0.4937 - val_loss: 1.8766 - val_acc: 0.4185\n",
      "Epoch 2/25\n",
      "5976/5976 [==============================] - 4656s 779ms/step - loss: 1.9973 - acc: 0.6500 - val_loss: 1.8531 - val_acc: 0.4376\n",
      "Epoch 3/25\n",
      "5976/5976 [==============================] - 1574s 263ms/step - loss: 1.6737 - acc: 0.7039 - val_loss: 2.2226 - val_acc: 0.4089\n",
      "Epoch 4/25\n",
      "5976/5976 [==============================] - 1611s 270ms/step - loss: 1.4416 - acc: 0.7433 - val_loss: 1.9573 - val_acc: 0.4819\n",
      "Epoch 5/25\n",
      "5976/5976 [==============================] - 1587s 266ms/step - loss: 1.2707 - acc: 0.7741 - val_loss: 3.1500 - val_acc: 0.3366\n",
      "Epoch 6/25\n",
      "5976/5976 [==============================] - 1594s 267ms/step - loss: 1.1576 - acc: 0.7938 - val_loss: 2.5051 - val_acc: 0.4402\n",
      "Epoch 7/25\n",
      "5976/5976 [==============================] - 1588s 266ms/step - loss: 1.0841 - acc: 0.8103 - val_loss: 2.9841 - val_acc: 0.4636\n",
      "Epoch 8/25\n",
      "5976/5976 [==============================] - 1604s 268ms/step - loss: 0.9950 - acc: 0.8234 - val_loss: 2.2324 - val_acc: 0.4850\n",
      "Epoch 9/25\n",
      "5976/5976 [==============================] - 1653s 277ms/step - loss: 0.9152 - acc: 0.8393 - val_loss: 2.3748 - val_acc: 0.4813\n",
      "Epoch 10/25\n",
      "5976/5976 [==============================] - 1598s 267ms/step - loss: 0.8710 - acc: 0.8493 - val_loss: 2.8401 - val_acc: 0.4488\n",
      "Epoch 11/25\n",
      "5976/5976 [==============================] - 1579s 264ms/step - loss: 0.8119 - acc: 0.8592 - val_loss: 2.3936 - val_acc: 0.5087\n",
      "Epoch 12/25\n",
      "5976/5976 [==============================] - 1574s 263ms/step - loss: 0.7670 - acc: 0.8661 - val_loss: 6.5337 - val_acc: 0.2504\n",
      "Epoch 13/25\n",
      "5976/5976 [==============================] - 1551s 260ms/step - loss: 0.7363 - acc: 0.8724 - val_loss: 2.7811 - val_acc: 0.4483\n",
      "Epoch 14/25\n",
      "5976/5976 [==============================] - 1564s 262ms/step - loss: 0.7005 - acc: 0.8776 - val_loss: 2.9336 - val_acc: 0.4500\n",
      "Epoch 15/25\n",
      "5976/5976 [==============================] - 1569s 263ms/step - loss: 0.6669 - acc: 0.8841 - val_loss: 2.5301 - val_acc: 0.4637\n",
      "Epoch 16/25\n",
      "5976/5976 [==============================] - 1573s 263ms/step - loss: 0.6237 - acc: 0.8899 - val_loss: 4.3627 - val_acc: 0.4235\n",
      "Epoch 17/25\n",
      "5976/5976 [==============================] - 1558s 261ms/step - loss: 0.5990 - acc: 0.8962 - val_loss: 2.7147 - val_acc: 0.5104\n",
      "Epoch 18/25\n",
      "5976/5976 [==============================] - 1576s 264ms/step - loss: 0.5817 - acc: 0.8997 - val_loss: 3.0630 - val_acc: 0.4671\n",
      "Epoch 19/25\n",
      "5976/5976 [==============================] - 1568s 262ms/step - loss: 0.5652 - acc: 0.9038 - val_loss: 2.8267 - val_acc: 0.4743\n",
      "Epoch 20/25\n",
      "5976/5976 [==============================] - 1576s 264ms/step - loss: 0.5435 - acc: 0.9068 - val_loss: 3.2553 - val_acc: 0.4754\n",
      "Epoch 21/25\n",
      "5976/5976 [==============================] - 1554s 260ms/step - loss: 0.5205 - acc: 0.9110 - val_loss: 4.5972 - val_acc: 0.2538\n",
      "Epoch 22/25\n",
      "5976/5976 [==============================] - 1565s 262ms/step - loss: 0.4964 - acc: 0.9146 - val_loss: 3.9297 - val_acc: 0.4364\n",
      "Epoch 23/25\n",
      "5976/5976 [==============================] - 1566s 262ms/step - loss: 0.4867 - acc: 0.9171 - val_loss: 2.8367 - val_acc: 0.5153\n",
      "Epoch 24/25\n",
      "5976/5976 [==============================] - 1563s 262ms/step - loss: 0.4795 - acc: 0.9182 - val_loss: 3.2496 - val_acc: 0.4523\n",
      "Epoch 25/25\n",
      "5976/5976 [==============================] - 1532s 256ms/step - loss: 0.4720 - acc: 0.9200 - val_loss: 2.9406 - val_acc: 0.4802\n"
     ]
    }
   ],
   "source": [
    "# Model fitting and training run\n",
    "start_time = datetime.datetime.now()\n",
    "print >>f, \"Training Start Time:%s \" % start_time \n",
    "#inception_10_tile_class_model=model.fit_generator(train_generator, steps_per_epoch= ntraining/batch_size, class_weight=class_weight,epochs=epochs, \\\n",
    "#    validation_data=val_generator,use_multiprocessing=True, workers=16, max_queue_size=32)\n",
    "\n",
    "\n",
    "inception_10_tile_class_model=parallel_model.fit_generator(train_generator, steps_per_epoch= ntraining // batch_size, class_weight=class_weight, \\\n",
    "                                                           epochs= epochs, validation_data=val_generator, use_multiprocessing=True, workers=16, max_queue_size=32)\n",
    "end_time = datetime.datetime.now()\n",
    "print >>f, \"Training took %s to finish %s epochs\" % ((end_time - start_time), epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p /output/results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model into a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save timestamped model to modelfilename\n",
    "modelfilename='10_tile_class_inception_model_'+filetime+'.h5'\n",
    "model.save(modelfilename)\n",
    "\n",
    "# Save model into .csv file\n",
    "hist = inception_10_tile_class_model.history\n",
    "hist = pd.DataFrame(hist)\n",
    "hist.to_csv('10_tile_class_inception_model_'+filetime+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model on Test Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on Test Images\n",
    "nTest = 0\n",
    "for root, dirs, files in os.walk(test_dir):\n",
    "    nTest += len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35359 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Testing Data Generator:\n",
    "# modified to suit histology images - 17th Aug, 2018\n",
    "#test_datagen = ImageDataGenerator(rescale=1. /255.)\n",
    "#testing_generator_noShuffle = test_datagen.flow_from_directory(\n",
    "#    test_dir,\n",
    "#    target_size=(img_width, img_height),\n",
    "#    batch_size=batch_size,\n",
    "#    shuffle=False,\n",
    "#    class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "testing_generator_noShuffle = test_datagen.flow_from_directory(\n",
    "    test_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=colormode,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_Validation: narray\n",
    "# row= image\n",
    "# column= probability of falling within label matching column_index\n",
    "predict_Testing = model.predict_generator(testing_generator_noShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Prediction for all labels\n",
    "best_prediction_per_label= [ max( predict_Testing[:,j] ) for j in range( predict_Testing.shape[1] ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicted label for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels= []\n",
    "# Find highest probability in prediction list for each image\n",
    "for i in predict_Testing:\n",
    "    i= list(i)\n",
    "    max_value = max(i) \n",
    "    predicted_labels.append( i.index(max_value) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix in a Plot\n",
    "import matplotlib.pyplot as pyplot\n",
    "pyplot.figure\n",
    "target_names = testing_generator_noShuffle.class_indices\n",
    "pyplot.figure(figsize=(10,10))\n",
    "cnf_matrix = confusion_matrix(testing_generator_noShuffle.classes, predicted_labels)\n",
    "classes = list(target_names)\n",
    "pyplot.imshow(cnf_matrix, interpolation='nearest')\n",
    "pyplot.colorbar()\n",
    "tick_marks = np.arange(len(classes))  \n",
    "_ = pyplot.xticks(tick_marks, classes, rotation=90)\n",
    "_ = pyplot.yticks(tick_marks, classes)\n",
    "plotopt= 'inception_10_tile_class_model_'+filetime+'.png'\n",
    "pyplot.savefig(plotopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_rpt = classification_report(testing_generator_noShuffle.classes, predicted_labels, target_names= testing_generator_noShuffle.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.60      0.67      0.63      3476\n",
      "       dlbc       0.73      0.47      0.57      9292\n",
      "       cesc       0.55      0.80      0.65      4096\n",
      "       read       0.53      0.46      0.49      5151\n",
      "        gbm       0.54      0.31      0.40      1055\n",
      "       chol       0.38      0.68      0.49      1771\n",
      "        lgg       0.56      0.77      0.64      1205\n",
      "       blca       0.76      0.99      0.86      3949\n",
      "       brca       0.75      0.66      0.70      4364\n",
      "       coad       0.17      0.09      0.11      1000\n",
      "\n",
      "avg / total       0.63      0.61      0.60     35359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cls_rpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning into classification report into classification object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgresults = cls_rpt.strip().split('\\n')[-1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallResults={'label' : 'avg/total', 'precision': avgresults[3], 'recall':avgresults[4],'f1-score':avgresults[5], 'support':avgresults[6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support  =  precision_recall_fscore_support(testing_generator_noShuffle.classes, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prediction object for each test image with filename, actual image label, Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import keras.preprocessing.image as Kimg\n",
    "\n",
    "modelInfo['imgprediction'] = []\n",
    "\n",
    "model=load_model(modelfilename)\n",
    "\n",
    "for fld in os.listdir('/data/test/'): \n",
    "    trueLabel = fld\n",
    "    for img in os.listdir('/data/test/%s/' %trueLabel): \n",
    "        imgPath = \"/data/test/%s/%s\" % (fld, img)\n",
    "        x = Kimg.load_img(imgPath, target_size=(64,64))\n",
    "        x = Kimg.img_to_array(x)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        x = x/255.\n",
    "        pr=model.predict(x)\n",
    "        curr = {'filename': img, 'actualImageLabel': fld, 'modelprediction':pr} \n",
    "        modelInfo['imgprediction'].append(curr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['imgprediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generates Top3 Predicted images for each individual image in the \"test folder\" only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as pyplot\n",
    "rows = 2\n",
    "cols = 2\n",
    "img_width, img_height = 64, 64\n",
    "pyplot.figure()\n",
    "pyplot.show()\n",
    "fig, ax = pyplot.subplots(rows, cols, frameon=False, figsize=(5, 5))\n",
    "fig.suptitle('Prediction Images', fontsize=10, y = 1.03)\n",
    "count=0\n",
    "\n",
    "modelInfo['imgprediction'] = []\n",
    "\n",
    "model=load_model(modelfilename)\n",
    "\n",
    "for fld in os.listdir('/data/test/'): \n",
    "    trueLabel = fld          \n",
    "    for imgname in os.listdir('/data/test/%s/' %trueLabel): \n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                imgPath = \"/data/test/%s/%s\" % (fld, imgname)                \n",
    "                img = Image.open(imgPath)\n",
    "                img = img.resize((img_width, img_height), Image.ANTIALIAS)\n",
    "                ax[i][j].imshow(img)\n",
    "                img = img_to_array(img)\n",
    "                img = img/255.0\n",
    "                img = img.reshape((1,) + img.shape)\n",
    "                pr = model.predict(img, batch_size= 1)                \n",
    "                curr = {'filename': img, 'actualImageLabel': fld, 'modelprediction':pr} \n",
    "                modelInfo['imgprediction'].append(curr)                \n",
    "                # To show image with top 3 predicted images\n",
    "                pred = pd.DataFrame(np.transpose(np.round(pr, decimals = 3)))\n",
    "                pred = pred.nlargest(n = 3, columns = 0) \n",
    "                pred['char'] = [list(modelInfo['labelname_to_index'].keys())[list(modelInfo['labelname_to_index'].values()).index(x)] for x in pred.index]\n",
    "                charstr = ''\n",
    "                for k in range(0,3):\n",
    "                    if k < 2:\n",
    "                        charstr = charstr+str(pred.iloc[k,1])+': '+str(pred.iloc[k,0])+'\\n'\n",
    "                    else:\n",
    "                        charstr = charstr+str(pred.iloc[k,1])+': '+str(pred.iloc[k,0])                \n",
    "                ec = (0, .8, .1)\n",
    "                fc = (0, .9, .2)\n",
    "                count = count + 1\n",
    "                ax[i][j].text(0, -10, charstr, size=10, rotation=0,\n",
    "                      ha=\"left\", va=\"top\", \n",
    "                      bbox=dict(boxstyle=\"round\", ec=ec, fc=fc, alpha = 0.7))\n",
    "                pyplot.setp(ax, xticks=[], yticks=[])\n",
    "                pyplot.tight_layout(rect=[0, 0.14, 1, 0.95])\n",
    "                pyplot.savefig('/data/code/results/prediction_'+ str(imgname) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
